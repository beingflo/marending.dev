---
title: 'Exploring LLMs'
subtitle: 'A skeptic reconsiders'
date: '20 Jul 2025'
link: '/notes/llm/'
layout: 'src/layouts/NoteLayout.astro'
draft: true
---

## Scope

Learn technical details of how LLMs work

Explore modern LLM tools and agents
- Newest OpenAI models
- Newest Anthropic models
- Newest Google models

Explore modern local models
-> Check leaderboards

Explore all models on the following tasks
- Coding
- Proof reading
- Architecture discussions
- Search (replacement for google)

Try local modal on my jour data set.

Bonus: Look into fine-tuning and deploying LLMs

Main goal is to acquire an informed view on LLMs, their uses and their limitations.

## Work log

### Notes on [Deep Dive into LLMs like ChatGPT](https://www.youtube.com/watch?v=7xTGNNLPyMI)

- Base models don't know about question / answer format, they simply predict the next token
from the context.
- One can fake a conversational agent by constructing a few-shot example prompt that shows the
question / answer format ending in `answer: `. The model will continue in a similar fashion.
- Instruct models are post-trained on human-generated conversations to properly bake in the
concept of answering questions. While the base training might take months, post-training typically
takes only hours.
- Hallucinations are being addressed by first probing a model for what it knows: Paste a paragraph
of wikipedia into one model and have it generate question / answer pairs related to the paragraph.
This is easy because the answers are right there in the context window. Take the generated questions
to another model and have it answer without having the context. Compare answers to determine if second
model knows the answer. If model doesn't know, add an answer to post-training data set to give answer
"Don't know".
- Tool usage works as follows: Model generates special tokens for e.g. web search, inference code
pauses when it encounters those tokens, goes of to do the search, pastes response back into context
and continues inference.
- Interesting insight on reasoning: There is only a fixed amount of computation flowing into any 
given token. So training data needs to be phrased in such a way that the model doesn't have to
solve the entire problem in one token, but instead can spread out intermediary computation on more
tokens. Kind of like slowly reasoning your way towards the solution rather than produce an answer 
immediately and then post-hoc justifying the answer. This leads to the conclusion that LLMs need
token to think. Thinking is intrinsically linked to language processing.
- Many counting and spelling tasks are not working well due to tokens. Tokens only exist for efficiency,
there are efforts to move to character-level or byte-level models. That may solve this problem.
- Post-processing: Reinforcement learning: Model is sampled many times on same question. Answers are
checked for correctness against known answer. Correct answers are trained on to make those token streams
more likely (reinforced). E.g. model has reasoned its way through problem well, not pinning too much computation
on a single token and thus got to the right answer. This is good and should be encouraged through fine-tuning
on those answers.
- Deepseek R1 was the first model where the use of reinforcement learning was publicly talked about.
This model learned to generate thinking sequences, where multiple different approaches are generated
and compared by the model. Only then does it generate a nice output intended for the human with the
previous thinking in context.
- together.ai for trying open weights models
- Reinforcement learning in *verifiable domains*: There is a way to reliably tell what the correct
answer is. Trivially or via LLM judge.
- RLHF: Reinforcement learning from human feedback
- Reinforcement learning in *unverifiable domains* (e.g. jokes): Train rewards model few little human
feedbacks, then use that to judge many outputs.
- Discriminator-generator gap: For a human it's way easier to judge output than create output.
- Risk of RLHF: RL may discover a way to game the model as a lossy simulation of a human is judging
output. That may lead to nonsensical output getting high scores. (Adversarial example)
- As a result RLHF is often run briefly to avoid this problem.
- Upcoming capabilities: Multimodal models: Tokenize audio, video and train as usual.
- Upcoming capabilities: Agents: Long, coherent, error-correcting contexts.
- Upcoming capabilities: Test-time training: Current models only do in-context learning on the context.

### Notes on [How I use LLMs](https://www.youtube.com/watch?v=EWvNQjAaOHw)

- Keep context on-topic. Large context may distract model and slightly decrease accuracy.
- Thinking (reasoning) models are considered ones that have been improved with reinforcement learning.
- GPT-4o is non-thinking model.
- OpenAI models starting with `o` (o3-mini, o1, etc.) are thinking models.
- Search tool use very useful to quickly gather websites and summarize content.
- Deep research combines search and thinking to do research.

### Notes on [Transformers, the tech behind LLMs](https://www.youtube.com/watch?v=wjZofJX0v4M&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=6)

- "Attention is all you need" originally introduced transformers.
- GPT-3 has 175B parameters
- Word embedding in GPT-3 is 12k-dimensional
- Typical word-embedding may choose to encode gender information in one dimension. Thus,
the classic `man - woman = uncle - aunt`. Similarly, `hitler + italy - germany = mussolini`.
- Further, one direction in the embedding space indicates _plurality_. i.e. `plur = cats - cat` and
the dot product between plurals and the `plur` direction is higher than with singular words.
- Embedding matrix in GPT-3 has around 617M weights.
- Embedding maps every token to a vector in isolation. It's the attention mechanism that enables
the network to exchange meaning _between_ the vectors, to arrive at a more enriched meaning for
each token. e.g. `river bank` vs `deposit at the bank`, `bank` has different meaning based on context.
- At the end, the _unembedding matrix_ maps the last token in the context to a probability distribution
across the entire vocabulary (50k in GPT-3) for prediction.
- All the other tokens in the last layer are actually encoding their immediately following tokens and
are not used for the prediction of a new token. This turns out to favor training.
- Unembedding matrix also has 617M weighs in GPT-3.
- Softmax is used to turn embedding vector into a probability distribution. The temperature is an extra
parameter here to guide how "sharp" the distribution is. i.e `T = 0`, highest component gets 100% of the
probability, `T = 5`, spreads out the probability much more evenly, thus making predictions more likely
that have lower components in the vector.
- Components in the output vector after unembedding are called Logits.

### Notes on [Attention in transformers, step-by-step](https://www.youtube.com/watch?v=eMlx5fFNoYc&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=7)

- Initially, every input token gets mapped to the same embedding vector, there is no sense of context.
- One interaction of attention may be to update the embeddings of nouns with the proceeding adjectives. This should refine the embedding
vector to one that captures the essence of the noun in context better. (this assumes tokens are words).
- Query and key vectors determine how relevant each token is to each other. i.e. the embedding of one word _attends_ to the
embedding of another.
- The dot products of each query vector with each key vector yields the _attention pattern_.
- To improve training, not only is the last token expected to predict the next one, but all tokens are expected to predict the following.
- _Masking_ is the process of setting the lower left half of the attention pattern to 0, to ensure later tokens can not "give away"
the prediction of earlier ones.
- Size of attention pattern is bottle neck for context window as it scales with the square.
- Finally, embeddings of tokens are updated by multiplying attention pattern by value vectors (produced using a value matrix). This way,
every token gets _some_ part of every other token (proceeding it), but more from the ones that are attending it more strongly.
- This whole process is called one _head of attention_.
- One attention head using approximately 6.4M weights.
- Multi-headed attention is running multiple attention layers in parallel. 96 per block for GPT-3.
- All deltas of each attention layer is added together to the original embedding.
- GPT-3 has 96 attention blocks, leading to 58B weights dedicated to attention.
- The rest of the 175B parameters are in the in-between multilayer perceptron layers.

### Easy way to run local models

- ollama
- lm studio
- msty
- open webui