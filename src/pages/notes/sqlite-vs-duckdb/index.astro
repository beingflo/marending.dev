---
import Layout from '../../../layouts/layout.astro';
import { A, Note, Title, P, H2, H3, C, Code, Info, Details } from '../../../components';
import { notes } from '../../notes';
import DuckDBResults from './DuckDBResults.astro';
import SQLiteResults from './SQLiteResults.astro';

const info = notes['sqlite-vs-duckdb'];
---

<Layout>
	<Note>
		<Title
			subtitle={info.subtitle}
			date={info.date}>
			{info.title}
		</Title>
		<P>
			I've been yearning to build my own observability / time series system. You can get some more
			background on this <A href="/notes/unstructured-data/">here</A>. After establishing
			<em>how</em> I'm going to store data, the question remaining is what DB I'm going to be using.
			In this note I'll evaluate two embedded databases, the venerable SQLite and the newcomer in this
			space, DuckDB.
		</P>
		<H2>Methodology</H2>
		<P>
			I'm populating the DB under test with mock data of three types: Location, CO2 and structured
			logs. This mirrors the scenarios from <A href="/notes/unstructured-data/">the last note</A>.
			The dataset includes around 50k location entries, 500k CO2 values and 90k structured logs with
			their timestamps nicely spread out over one year.
		</P>
		<Details title="Full population script for reference">
			<Code
				respectMargin
				value=`import { faker } from "@faker-js/faker";
import * as Throttle from "promise-parallel-throttle";

console.log("deleting ...");
await fetch("http://localhost:3000/delete", {
  method: "POST",
});
console.log("deleted");

faker.seed(123);

const startDate = new Date("01/01/2023");
const endDate = new Date("12/31/2023");
let curDate = structuredClone(startDate);

const payloads: Array<any> = [];

// Location - 10m
while (curDate.getTime() < endDate.getTime()) {
  const payload = {
    data: {
      longitude: faker.location.longitude(),
      latitude: faker.location.latitude(),
    },
    bucket: "location",
    timestamp: curDate.toISOString(),
  };
  payloads.push(payload);
  curDate = new Date(curDate.getTime() + 600_000);
}

// CO2 - 1m
curDate = structuredClone(startDate);

while (curDate.getTime() < endDate.getTime()) {
  const payload = {
    data: {
      co2: faker.number.float({ min: 0, max: 5000 }),
    },
    bucket: "co2",
    timestamp: curDate.toISOString(),
  };
  payloads.push(payload);
  curDate = new Date(curDate.getTime() + 60_000);
}

// Structured logs - 1h bursty
curDate = structuredClone(startDate);

while (curDate.getTime() < endDate.getTime()) {
  let count = 10;
  const user = faker.internet.userName();
  const endpoint = faker.internet.url();
  while (count > 0) {
    const payload = {
      data: {
        span_id: faker.number.int(),
        level: faker.datatype.boolean(0.95) ? "success" : "error",
        user,
        message: faker.company.buzzPhrase(),
        endpoint,
      },
      bucket: "logs",
      timestamp: curDate.toISOString(),
    };
    payloads.push(payload);
    curDate = new Date(curDate.getTime() + 100);
    count -= 1;
  }
  curDate = new Date(curDate.getTime() + 3_600_000);
}

payloads.sort(
  (a, b) => new Date(a.timestamp).getTime() - new Date(b.timestamp).getTime()
);

const doRequest = async (payload, idx) => {
  await fetch("http://localhost:3000", {
    method: "POST",
    body: JSON.stringify(payload),
    headers: { "Content-Type": "application/json" },
  });
};

const queued = payloads.map((p, idx) => () => doRequest(p, idx));

await Throttle.all(queued, { maxInProgress: 10 });`
			/>
		</Details>
		<P>
			SQLite is operated in WAL mode with synchronization <C>Normal</C>, while DuckDB uses no
			special configuration as WAL mode is enabled by default.
		</P>
		<P>
			The schema used in the tests looks as follows:
			<Code
				value=`CREATE TABLE IF NOT EXISTS metrics (
	id integer primary key,
	timestamp TIMESTAMP NOT NULL,
	bucket TEXT NOT NULL,
	data JSON NOT NULL
);`
			/>
			Notably, due to SQLite version shenanigans I couldn't use
			<A href="https://sqlite.org/draft/jsonb.html">JSONB</A>.
		</P>
		<H3>Queries</H3>
		<P>
			Next, to exercise the DB, I'm querying the three categories of data with fairly intensive
			queries.
		</P>
		<P>GPS coordinates:</P>
		<Code
			value=`SELECT data ->> '$.longitude', data ->> '$.latitude' FROM metrics WHERE bucket = 'location' AND data ->> '$.longitude' > 6 AND data ->> '$.longitude' < 10 AND data ->> '$.latitude' > 45 AND data ->> '$.latitude' < 50;`
		/>
		<P>
			This checks how many location entries are within a certain longitude and latitude rectangle.
		</P>
		<P>CO2:</P>
		<Code
			value=`SELECT strftime('%m', timestamp) as timestamp, avg(data -> '$.co2') as avg FROM metrics WHERE bucket = 'co2' GROUP BY strftime('%m', timestamp);`
		/>
		<P>
			Possibly the most intensive query: This aggregates CO2 measurements by month and computes the
			respective averages.
		</P>
		<P>Structured logs:</P>
		<Code
			value=`SELECT count(*), data ->> '$.endpoint' FROM metrics WHERE bucket = 'logs' AND data ->> '$.level' = 'error' AND timestamp > DATE('2024-01-01', '-90 day') GROUP BY data ->> '$.endpoint' ORDER BY count(*);`
		/>
		<P>
			This groups the entries indicating an error by <C>endpoint</C> and checks which one has the highest
			error count. It does so over the data of the last 90 days.
		</P>
		<P>
			Note that the queries look slightly different between SQLite and DuckDB, in particular with
			date functions and json field access patterns.
		</P>
		<H2>Results</H2>
		<DuckDBResults />
		<SQLiteResults />
		<P>
			Clearly, DuckDB is faster across the board. Interestingly, unlike SQLite, DuckDB doesn't
			benefit from indices at all, they even seem to hurt performance. The structured logs query
			benefits particularly from the timestamp index on SQLite, as it is the only query that filters
			data based on the timestamp.
		</P>
		<P>
			While not surprising at first glance that a column-oriented DB is faster in such workloads,
			one has to keep in mind that most of the computation happens on fields within a JSON blob. I
			would expect that most of the benefit of neat data alignment, ready for vectorized
			computation, is forfeit in this situation. But clearly DuckDB still manages to eke out some
			performance benefits.
		</P>
		<H2>Inserts</H2>
		<P>
			DuckDB is not all sunshine and roses though. From what I'm gathering, their design goal is
			primarily focused on bulk operations, not so much individual transactions. This shows in the
			insert rate: DuckDB achieves around 4k naive inserts per second, while SQLite manages around
			30k to 40k depending on indices. This can be remedied somewhat with the
			<A href="https://duckdb.org/docs/data/appender.html">Appender API</A> though. Alternatively I've
			been working around it by buffering writes in memory and doing batched inserts "off-thread". This
			has the added advantage of being able to soak up peaks of around 100k inserts per second.
		</P>
		<H2>DB Size</H2>
		<P>DuckDB 20 MB, SQLite 60 - 80 MB.</P>
		<H2>Other considerations</H2>
		<P>duckdb takes a long time to open db when WAL file large, checkpoint can keep it in check</P>
		<P>sqlite with WAL and synchro normal</P>
	</Note>
</Layout>
